#!/usr/bin/env python3
"""Main battle script. Executes all possible types of battles, see battle --help for all options."""
from __future__ import annotations
import sys
import os
import logging

from argparse import ArgumentParser
import datetime as dt
from pathlib import Path

import algobattle
from algobattle.match import Match
from algobattle.util import import_problem_from_path
from algobattle.docker import measure_runtime_overhead
from algobattle.ui import Ui

#! do not remove, even when unused!
import algobattle.sighandler

def setup_logging(logging_path: Path, verbose_logging: bool, silent: bool):
    """Creates and returns a parent logger.

    Parameters:
    ----------
    logging_path : Path
        Path to folder where the logfile should be stored at.
    verbose_logging : bool
        Flag indicating whether to include debug messages in the output
    silent : bool
        Flag indicating whether not to pipe the logging output to stderr.

    Returns:
    ----------
    Logger:
        The Logger object.
    """
    common_logging_level = logging.INFO

    if verbose_logging:
        common_logging_level = logging.DEBUG

    logging_path.mkdir(parents=True, exist_ok=True)

    _now = dt.datetime.now()

    time_seperator = ':' if os.name == 'posix' else '-'
    current_timestamp = f'{_now.year:04d}-{_now.month:02d}-{_now.day:02d}_{_now.hour:02d}{time_seperator}{_now.minute:02d}{time_seperator}{_now.second:02d}'
    logging_path /= current_timestamp + '.log'

    logging.basicConfig(handlers=[logging.FileHandler(logging_path, 'w', 'utf-8')],
                        level=common_logging_level,
                        format='%(asctime)s %(levelname)s: %(message)s',
                        datefmt='%H:%M:%S')

    logger = logging.getLogger('algobattle')

    if not silent:
        # Pipe logging out to console
        _consolehandler = logging.StreamHandler(stream=sys.stderr)
        _consolehandler.setLevel(common_logging_level)

        _consolehandler.setFormatter(logging.Formatter('%(message)s'))

        logger.addHandler(_consolehandler)

    logger.info(f'You can find the log files for this run in {logging_path}')
    return logger

def resolved_path(path: str) -> Path:
    return Path(path).resolve()

if __name__ == "__main__":
    default_logging_path = Path.home() / '.algobattle_logs'
    default_config = Path(algobattle.__file__).resolve().parent / 'config' / 'config.ini'

    parser = ArgumentParser(prog="algobattle")
    parser.add_argument("problem_path", type=resolved_path, help="Path to the parent directory of a problem file as argument.")
    parser.add_argument("--version", action="version", version=f"Algobattle version {algobattle.__version__}")
    parser.add_argument('--verbose', dest='verbose_logging', action='store_true', help='Log all debug messages.')
    parser.add_argument('--silent', dest='silent', action='store_true', help='Disable forking the logging output to stderr.')
    parser.add_argument('--ui', dest='display_ui', action='store_true', help='If set, the program sets the --silent option and displays a small ui on STDOUT that shows the progress of the battles.')
    parser.add_argument('--output_folder', dest='folder_name', type=resolved_path, default=default_logging_path, help='Specify the folder into which the log file is written to. Can either be a relative or absolute path to folder. If nonexisting, a new folder will be created. Default: ~/.algobattle_logs/')
    parser.add_argument('--config_file', dest='config', type=resolved_path, default=default_config, help='Path to a .ini configuration file to be used for the run. Defaults to the packages config.ini')
    parser.add_argument('--no_overhead_calculation', dest='no_overhead_calculation', action='store_true', help='If set, the program does not benchmark the I/O of the host system to calculate the runtime overhead when started.')

    parser.add_argument('--solvers', dest='solvers', default="{problem_path}/solver", help='Specify the folder names containing the solvers of all involved teams as a comma-seperated list. Default: arg1/solver/')
    parser.add_argument('--generators', dest='generators', default="{problem_path}/generator", help='Specify the folder names containing the generators of all involved teams as a comma-seperated list. Default: arg1/generator/')
    parser.add_argument('--team_names', dest='team_names', default='0', help='Specify the team names of all involved teams as a list strings as a comma-seperated list. Default: "0"')

    parser.add_argument('--battle_type', dest='battle_type', choices=['iterated', 'averaged'], default='iterated', help='Type of battle wrapper to be used. Possible options: iterated, averaged. Default: iterated')
    parser.add_argument('--rounds', dest='battle_rounds', type=int, default='5', help='Number of rounds that are to be fought in the battle (points are split between all rounds). Default: 5')
    parser.add_argument('--points', dest='points', type=int, default='100', help='Number of points that are to be fought for. Default: 100')
    parser.add_argument('--do_not_count_points', dest='do_not_count_points', action='store_true', help='If set, points are not calculated for the run.')

    parser.add_argument('--iter_cap', dest='iterated_cap', type=int, default='50000', help='If --battle_type=iterated, the maximum instance size up to which a battle is to be fought. Default: 50000')
    parser.add_argument('--iter_exponent', dest='iterated_exponent', type=int, default='2', help='If --battle_type=iterated, the exponent used for the step size increase. Default: 2')
    parser.add_argument('--approx_ratio', dest='approximation_ratio', type=float, default='1.0', help='Tolerated approximation ratio of a solution, if the problem is compatible with approximation. Default: 1.0')

    parser.add_argument('--approx_inst_size', dest='approximation_instance_size', type=int, default='10', help='If --battle_type=averaged, the instance size on which the averaged run is to be made. Default: 10')
    parser.add_argument('--approx_iterations', dest='approximation_iterations', type=int, default='25', help='If --battle_type=averaged, the number of iterations that are to be averaged. Default: 25')

    options = parser.parse_args()

    display_ui = options.display_ui

    if display_ui:
        options.silent = True

    problem_path = Path(options.problem_path)
    solvers = options.solvers.format(problem_path=str(problem_path))
    solvers = [Path(s).resolve() for s in solvers.split(',')]
    generators = options.generators.format(problem_path=str(problem_path))
    generators = [Path(g).resolve() for g in generators.split(',')]
    team_names = options.team_names.split(',')

    if not (len(solvers) == len(generators) == len(team_names)):
        sys.exit(f'The number of provided generator paths ({len(generators)}), solver paths ({len(solvers)}) and group numbers ({len(team_names)}) is not equal!')

    if not problem_path.exists():
        sys.exit(f'Problem path "{problem_path}" does not exist in the file system! Use "battle --help" for more information on usage and options.')
    if not options.config.exists():
        sys.exit(f'Config path "{options.config}" does not exist in the file system! Use "battle --help" for more information on usage and options.')
    for solver_path in solvers:
        if not solver_path.exists():
            sys.exit(f'The given path for option --solvers "{solver_path}" does not exist in the file system! Use "battle --help" for more information on usage and options.')
    for generator_path in generators:
        if not generator_path.exists():
            sys.exit(f'The given path for option --generators "{generator_path}" does not exist in the file system! Use "battle --help" for more information on usage and options.')

    logger = setup_logging(options.folder_name, options.verbose_logging, options.silent)

    problem = import_problem_from_path(problem_path)
    if not problem:
        sys.exit(1)

    teams = [(team_names[i], generators[i], solvers[i]) for i in range(len(generators))]

    logger.debug(f'Options for this run: {options}')
    logger.debug(f'Contents of sys.argv: {sys.argv}')

    runtime_overhead = 0
    if not options.no_overhead_calculation:
        logger.info('Running a benchmark to determine your machines I/O overhead to start and stop docker containers...')
        runtime_overhead = measure_runtime_overhead()
        logger.info(f'Maximal measured runtime overhead is at {runtime_overhead} seconds. Adding this amount to the configured runtime.')

    if display_ui:
        ui = Ui(logger, logger.getEffectiveLevel())
        ui.update("Setting up match...")
    else:
        ui = None
    
    match = Match(problem, options.config, teams, ui, runtime_overhead=runtime_overhead, approximation_ratio=options.approximation_ratio)


    results = match.run(options.battle_type, rounds=options.battle_rounds, iterated_cap=options.iterated_cap,
                        iterated_exponent=options.iterated_exponent, approximation_instance_size=options.approximation_instance_size,
                        approximation_iterations=options.approximation_iterations)

    if ui is not None:
        ui.restore()

    logger.info('#' * 78)
    logger.info(f'\n{results.format()}')
    if not options.do_not_count_points:
        points = results.calculate_points(options.points)
        for team, points in points.items():
            logger.info(f'Group {team} gained {points:.1f} points.')
    
    match.cleanup()
    print(results.format())
