#!/usr/bin/env python3
import sys
import os
import logging

from optparse import OptionParser
import datetime as dt
from pathlib import Path

import algobattle
from algobattle.match import Match
from algobattle.team import Team
from algobattle.util import calculate_points, measure_runtime_overhead, import_problem_from_path
from algobattle.ui import Ui

if __name__ == "__main__":

    def setup_logging(logging_path, verbose_logging, silent):
        """ Creates and returns a parent logger.

        Parameters:
        ----------
        logging_path : str
            Path where the logfile should be stored at.
        verbose_logging : bool
            Flag indicating whether to include debug messages in the output
        silent : bool
            Flag indicating whether not to pipe the logging output to stderr.

        Returns:
        ----------
        Logger:
            The Logger object.
        """
        common_logging_level = logging.INFO

        if verbose_logging:
            common_logging_level = logging.DEBUG

        if not os.path.exists(logging_path):
            os.makedirs(logging_path)

        _now = dt.datetime.now()
        current_timestamp = '{:04d}-{:02d}-{:02d}_{:02d}:{:02d}:{:02d}'.format(_now.year, _now.month, _now.day, _now.hour, _now.minute, _now.second)
        logging_path = logging_path + current_timestamp + '.log'

        logging.basicConfig(filename=logging_path, level=common_logging_level, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%H:%M:%S')

        logger = logging.getLogger('algobattle')

        if not silent:
            # Pipe logging out to console
            _consolehandler = logging.StreamHandler(stream=sys.stderr)
            _consolehandler.setLevel(common_logging_level)

            _consolehandler.setFormatter(logging.Formatter('%(message)s'))

            logger.addHandler(_consolehandler)

        logger.info('You can find the log files for this run in {}'.format(logging_path))
        return logger


    if len(sys.argv) < 2:
        sys.exit('Expecting (relative) path to the parent directory of a problem file as argument. Use "battle --help" for more information on usage and options.')

    problem_path = str(Path(sys.argv[1]).resolve())

    default_logging_path = str(Path.home()) + '/.algobattle_logs/'
    default_config = os.path.join(os.path.dirname(os.path.abspath(algobattle.__file__)), 'config', 'config.ini')

    # Option parser to process arguments from the console.
    usage = 'usage: %prog FILE [options]\nExpecting (relative) path to the parent directory of the problem file as first argument.\nIf you provide generators, solvers and group numbers for multiple teams, make sure that the order is the same for all three arguments!'
    parser = OptionParser(usage=usage, version=algobattle.__version__)
    parser.add_option('--verbose', dest='verbose_logging', action='store_true', help='Log all debug messages.')
    parser.add_option('--output_folder', dest='folder_name', default=default_logging_path, help='Specify the folder into which all logging files are written to. Default: ~/.algobattle_logs/')
    parser.add_option('--config_file', dest='config', default=default_config, help='Path to a .ini configuration file to be used for the run. Defaults to the packages config.ini')
    parser.add_option('--solvers', dest='solvers', default=problem_path + '/solver/', help='Specify the folder names containing the solvers of all involved teams as a comma-seperated list. Default: arg1/solver/')
    parser.add_option('--generators', dest='generators', default=problem_path + '/generator/', help='Specify the folder names containing the generators of all involved teams as a comma-seperated list. Default: arg1/generator/')
    parser.add_option('--team_names', dest='team_names', default='0', help='Specify the group names of all involved teams as a list strings as a comma-seperated list. Default: "0"')
    parser.add_option('--rounds', dest='battle_rounds', type=int, default='5', help='Number of rounds that are to be made in the battle (points are split between all rounds). Default: 5')
    parser.add_option('--battle_type', dest='battle_type', choices=['iterated', 'averaged'], default='iterated', help='Selected battle type. Possible options: iterated, averaged. Default: iterated')
    parser.add_option('--approx_ratio', dest='approximation_ratio', type=float, default='1.0', help='Tolerated approximation ratio for a problem, if compatible with approximation. Default: 1.0')
    parser.add_option('--approx_inst_size', dest='approximation_instance_size', type=int, default='10', help='If --battle_type=averaged, the instance size on which the averaged run is to be made. Default: 10')
    parser.add_option('--approx_iterations', dest='approximation_iterations', type=int, default='25', help='If --battle_type=averaged, the number of iterations that are to be averaged. Default: 25')
    parser.add_option('--iter_cap', dest='iterated_cap', type=int, default='50000', help='If --battle_type=iterated, the maximum instance size up to which a battle is to be fought.')
    parser.add_option('--points', dest='points', type=int, default='100', help='Number of points for which are fought. Default: 100')
    parser.add_option('--do_not_count_points', dest='do_not_count_points', action='store_true', help='If set, points are not calculated for the run.')
    parser.add_option('--silent', dest='silent', action='store_true', help='Disable forking the logging output to stderr.')
    parser.add_option('--no-overhead-calculation', dest='no_overhead_calculation', action='store_true', help='If set, the program does not benchmark the I/O of the host system to calculate the runtime overhead when started.')
    parser.add_option('--ui', dest='display_ui', action='store_true', help='If set, the program sets the --silent option and displays a small ui on STDOUT that shows the progess of the battles. Currently only works for iterated battles.')

    (options, args) = parser.parse_args()

    display_ui = options.display_ui

    if display_ui:
        options.silent = True

    solvers = options.solvers.split(',')
    generators = options.generators.split(',')
    team_names = options.team_names.split(',')

    if len(solvers) != len(generators) or len(solvers) != len(team_names) or len(team_names) != len(generators):
        sys.exit('The number of provided generator paths ({}), solver paths ({}) and group numbers ({}) is not equal!'.format(len(generators), len(solvers), len(team_names)))

    if not os.path.exists(problem_path):
        sys.exit('Problem path "{}" does not exist in the file system! Use "battle --help" for more information on usage and options.'.format(problem_path))
    if not os.path.exists(options.config):
        sys.exit('Config path "{}" does not exist in the file system! Use "battle --help" for more information on usage and options.'.format(options.config))
    for solver_path in solvers:
        if not os.path.exists(solver_path):
            sys.exit('The given path for option --solvers "{}" does not exist in the file system! Use "battle --help" for more information on usage and options.'.format(solver_path))
    for generator_path in generators:
        if not os.path.exists(generator_path):
            sys.exit('The given path for option --generators "{}" does not exist in the file system! Use "battle --help" for more information on usage and options.'.format(generator_path))

    logger = setup_logging(options.folder_name, options.verbose_logging, options.silent)

    problem = import_problem_from_path(problem_path)
    if not problem:
        sys.exit(1)

    teams = []
    for i in range(len(generators)):
        teams.append(Team(team_names[i], generators[i], solvers[i]))

    logger.debug('Options for this run: {}'.format(options))
    logger.debug('Contents of sys.argv: {}'.format(sys.argv))

    runtime_overhead = 0
    if not options.no_overhead_calculation:
        logger.info('Running a benchmark to determine your machines I/O overhead to start and stop docker containers...')
        runtime_overhead = measure_runtime_overhead()
        logger.info('Maximal measured runtime overhead is at {} seconds. Adding this amount to the configured runtime.'.format(runtime_overhead))

    match = Match(problem, options.config, teams, runtime_overhead=runtime_overhead, approximation_ratio=options.approximation_ratio)

    if not match.build_successful:
        logger.critical('Building the match object failed, exiting!')
        sys.exit(1)

    ui = None
    if display_ui:
        ui = Ui()
        match.attach(ui)

    results = match.run(options.battle_type, options.battle_rounds, options.iterated_cap, 
                        options.approximation_instance_size, options.approximation_iterations)

    if display_ui:
        match.detach(ui)

    logger.info('#'*78)
    if not options.do_not_count_points:
        points = calculate_points(results, options.points)

        for team_name in [team.name for team in teams]:
            logger.info('Group {} gained {:.1f} points.'.format(team_name, points[team_name]))
