#!/usr/bin/env python3
import sys
import os
import logging
import configparser
from optparse import OptionParser
import datetime as dt
import importlib.util
import pkgutil
from pathlib import Path
import subprocess

from algobattle.match import Match
from algobattle.team import Team
import algobattle.sighandler as sigh

# Timestamp format: YYYY-MM-DD_HH:MM:SS
_now = dt.datetime.now()
current_timestamp = '{:04d}-{:02d}-{:02d}_{:02d}:{:02d}:{:02d}'.format(_now.year, _now.month, _now.day, _now.hour, _now.minute, _now.second)

if len(sys.argv) < 2:
    sys.exit('Expecting (relative) path to the parent directory of the problem file as argument. Use ./run.py --help for more information on usage and options.')

docker_running = subprocess.Popen(['docker', 'info'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
_ = docker_running.communicate()
if docker_running.returncode:
    sys.exit('Could not connect to the docker daemon. Is docker running?')

package_path = '/' + str(Path(__file__).resolve())
if package_path.endswith('/scripts'):
    package_path = package_path [:-8]
problem_path = str(Path(sys.argv[1]).resolve())
default_logging_path = str(Path.home()) + '/.algobattle_logs/'

##Option parser to process arguments from the console.
usage = 'usage: ./%prog FILE [options]\nExpecting (relative) path to the parent directory of the problem file as first argument.\nIf you provide generators, solvers and group numbers for multiple teams, make sure that the order is the same for all three arguments!'
parser = OptionParser(usage=usage)
parser.add_option('--verbose', dest = 'verbose_logging', action = 'store_true', help = 'Log all debug messages.')
parser.add_option('--output_folder', dest = 'folder_name', default = default_logging_path, help = 'Specify the folder into which all logging files are written to. Default: ~/.algobattle_logs/')
parser.add_option('--config_file', dest = 'config_file', help = 'Path to a .ini configuration file to be used for the run. Defaults to the packages config.ini')
parser.add_option('--solvers', dest = 'solvers', default = problem_path + '/solver/', help = 'Specify the folder names containing the solvers of all involved teams as a comma-seperated list. Default: arg1/solver/')
parser.add_option('--generators', dest = 'generators', default = problem_path + '/generator/', help = 'Specify the folder names containing the generators of all involved teams as a comma-seperated list. Default: arg1/generator/')
parser.add_option('--group_numbers', dest = 'group_nrs', default = '0', help = 'Specify the group numbers all involved teams as a list of nonnegative integers as a comma-seperated list. Default: 0')
parser.add_option('--iterations', dest = 'battle_iterations', type=int, default = '5', help = 'Number of fights that are to be made in the battle (points are split between each fight). Default: 5')
parser.add_option('--battle_type', dest = 'battle_type', choices=['iterated', 'averaged'], default = 'iterated', help = 'Selected battle type. Possible options: iterated, averaged. Default: iterated')
parser.add_option('--approx_ratio', dest = 'approximation_ratio', type=float, default = '1.0', help = 'Tolerated approximation ratio for a problem, if compatible with approximation. Default: 1.0')
parser.add_option('--approx_inst_size', dest = 'approximation_instance_size', type=int, default = '10', help = 'If --battle_type=averaged, the instance size on which the averaged run is to be made . Default: 10')
parser.add_option('--approx_iterations', dest = 'approximation_iterations', type=int, default = '50', help = 'If --battle_type=averaged, number of iteration over which the approximation ratio is averaged. Default: 50')
parser.add_option('--points', dest = 'points', type=int, default = '100', help = 'Number of points for which are fought. Default: 100')
parser.add_option('--do_not_count_points', dest = 'do_not_count_points', action = 'store_true', help = 'If set, points are not calculated for the run.')
parser.add_option('-c', '--do_not_log_to_console', dest = 'do_not_log_to_console', action = 'store_true', help = 'Disable forking the logging output to stderr.')
parser.add_option('--no-overhead-calculation', dest = 'no_overhead_calculation', action = 'store_true', help = 'If set, the program does not benchmark the I/O of the host system to calculate the runtime overhead when started.')

(options, args) = parser.parse_args()

solvers = options.solvers.split(',')
generators = options.generators.split(',')
group_numbers = options.group_nrs.split(',')

if len(solvers) != len(generators) or len(solvers) != len(group_numbers) or len(group_numbers) != len(generators):
    sys.exit('The number of provided generator paths ({}), solver paths ({}) and group numbers ({}) is not equal!'.format(len(generators), len(solvers), len(group_numbers)))

if any(not group_nr.isdigit() for group_nr in group_numbers):
    sys.exit('The group numbers are expected to be nonnegative integers!')
group_numbers = [int(group_nr) for group_nr in group_numbers]

# Validate that all paths given by options exist
if not os.path.exists(problem_path):
    sys.exit('Input path "{}" does not exist in the file system! Use ./run.py --help for more information on usage and options.'.format(problem_path))
for solver_path in solvers:
    if not os.path.exists(solver_path):
        sys.exit('The given path for option --solver "{}" does not exist in the file system! Use ./run.py --help for more information on usage and options.'.format(solver_path))
for generator_path in generators:
    if not os.path.exists(generator_path):
        sys.exit('The given path for option --generator1 "{}" does not exist in the file system! Use ./run.py --help for more information on usage and options.'.format(generator_path))


# Logging level below which no logs are supposed to be written out.
common_logging_level = logging.INFO

# Enable logging of all levels if the option is set
if options.verbose_logging:
    common_logging_level = logging.DEBUG

if not os.path.exists(options.folder_name):
    os.makedirs(options.folder_name)

# Strings to build the logfile name
logging_path = options.folder_name + current_timestamp + '.log'

# Initialize logger
logging.basicConfig(filename=logging_path, level=common_logging_level, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%H:%M:%S')
# Parent-logger for the whole program
logger = logging.getLogger('algobattle')

# Pipe logging out to console if not disabled by option
if not options.do_not_log_to_console:
    _consolehandler = logging.StreamHandler(stream=sys.stderr)
    _consolehandler.setLevel(common_logging_level)

    _consolehandler.setFormatter(logging.Formatter('%(message)s'))

    logger.addHandler(_consolehandler)

logger.debug('Options for this run: {}'.format(options))
logger.debug('Contents of sys.argv: {}'.format(sys.argv))

# Read in config file specifying problem parameters.
config = configparser.ConfigParser()
if not options.config_file:
    logger.info('No config file provided, using the default config.ini')
    config_data = pkgutil.get_data('algobattle', 'config/config.ini').decode()
    config.read_string(config_data)
elif not os.path.isfile(options.config_file):
    logger.error('Config file at path "%s" could not be found, terminating!', options.config_file)
    sys.exit(1)
else:
    logger.info('Using additional configuration options from file "%s".', options.config_file)
    config.read(options.config_file)


def main():
    try:
        spec = importlib.util.spec_from_file_location("problem", problem_path + "/__init__.py")
        Problem = importlib.util.module_from_spec(spec)
        sys.modules[spec.name] = Problem
        spec.loader.exec_module(Problem)

        problem = Problem.Problem()
    except Exception as e:
        logger.critical('Importing the given problem failed with the following exception: "{}"'.format(e))
        sys.exit(1)

    if options.approximation_ratio != 1.0 and not problem.approximable:
        logger.critical('The selected problem cannot be approximated! Run it without the approximation ratio flag or pick a problem that is approximable!')
        sys.exit(1)

    teams = []
    for i in range(len(generators)):
        teams.append(Team(group_numbers[i], generators[i], solvers[i]))
    runtime_overhead = 0
    if not options.no_overhead_calculation:
        logger.info('Running a benchmark to determine your machines I/O overhead to start and stop docker containers...')
        runtime_overhead = calculate_time_tolerance()
        logger.info('Maximal measured runtime overhead is at {} seconds. Adding this amount to the configured runtime.'.format(runtime_overhead))
    match = Match(problem, config, teams, runtime_overhead=runtime_overhead, 
                    approximation_ratio=options.approximation_ratio, approximation_instance_size=options.approximation_instance_size,
                    approximation_iterations=options.approximation_iterations)

    if not match.build_successful:
        logger.critical('Building the match object failed, exiting!')
        sys.exit(1)

    results = match.run(options.battle_type, options.battle_iterations)

    logger.info('#'*70)
    if options.battle_iterations > 0:
        if options.battle_type == 'iterated':
            logger.info('Summary of the battle results: \n{}\n'.format(
                format_summary_message_iterated(results, match.all_battle_pairs())))
        elif options.battle_type == 'averaged':
            logger.info('Summary of the battle results: \n{}\n'.format(
                format_summary_message_averaged(results, match.all_battle_pairs())))
        else:
            logger.info('No summary message configured for this type of battle.')

        if not options.do_not_count_points:
            points = calculate_points(results, options.points, group_numbers, options.battle_iterations ,options.battle_type)



        for group_nr in group_numbers:
            logger.info('Group {} gained {} points.'.format(group_nr, points[group_nr]))

    print('You can find the log files for this run in {}'.format(logging_path))


def calculate_points(results, achievable_points, group_numbers, battle_iterations, battle_type):
    """ Calculate the number of achieved points, given results.
    """
    points = dict()
    # We want all groups to be able to achieve the same number of total points, regardless of the number of teams
    normalizer = (len(group_numbers) - 1) if len(group_numbers) > 1 else 1
    for i in range(len(group_numbers)):
        for j in range(i+1, len(group_numbers)):
            points[group_numbers[i]] = points.get(group_numbers[i], 0)
            points[group_numbers[j]] = points.get(group_numbers[j], 0)
            #Points are awarded for each match individually, as one run reaching the cap poisons the average number of points
            for k in range(battle_iterations):
                results[(group_numbers[i], group_numbers[j])][i]
                if battle_type == 'iterated':
                    valuation0 = results[(group_numbers[i], group_numbers[j])][k]
                    valuation1 = results[(group_numbers[j], group_numbers[i])][k]
                elif battle_type == 'averaged':
                    # The valuation of an averaged battle
                    # is the number of successfully executed battles divided by
                    # the average competitive ratio of successful battles,
                    # to account for failures on execution. A higher number
                    # thus means a better overall result. Normalized to the number of configured points.
                    valuation0 = len(results[(group_numbers[i], group_numbers[j])][k]) / (sum(results[(group_numbers[i], group_numbers[j])][k]) / len(results[(group_numbers[i], group_numbers[j])][k]))
                    valuation1 = len(results[(group_numbers[j], group_numbers[i])][k]) / (sum(results[(group_numbers[j], group_numbers[i])][k]) / len(results[(group_numbers[j], group_numbers[i])][k]))
                else:
                    logger.info('Unclear how to calculate points for this type of battle.')

                if valuation0 + valuation1 > 0:
                    points[group_numbers[i]] += ( (achievable_points/battle_iterations * valuation0) / (valuation0 + valuation1) ) // normalizer
                    points[group_numbers[j]] += ( (achievable_points/battle_iterations * valuation1) / (valuation0 + valuation1) ) // normalizer
                else:
                    points[group_numbers[i]] += ( (achievable_points/battle_iterations) // 2 ) // normalizer
                    points[group_numbers[j]] += ( (achievable_points/battle_iterations) // 2 ) // normalizer

    return points

def calculate_time_tolerance():
    """ Calculate the I/O delay for starting and stopping docker on the host machine.

        Returns:
        ----------
        float:
            I/O overhead in seconds.
    """
    Problem = importlib.import_module('algobattle.problems.delaytest')
    problem = Problem.Problem()

    delaytest_path = Problem.__file__[:-12] #remove /__init__.py
    delaytest_team = Team(0, delaytest_path + '/generator', delaytest_path + '/solver')
    match = Match(problem, config, [delaytest_team])

    if not match.build_successful:
        logger.warning('Building a match for the time tolerance calculation failed!')
        return 0
    overheads = []
    for i in range(10):
        sigh.latest_running_docker_image = "generator0"
        _, timeout = match._run_subprocess(match.base_build_command + ["generator0"], input=str(50*i).encode(), timeout=match.timeout_generator)
        overheads.append(float(timeout))

    max_overhead = round(max(overheads), 2)

    return max_overhead

def format_summary_message_iterated(results, all_battle_pairs):
    """ Format the results of a battle into a summary message.

        Parameters:
        ----------
        results0: list 
            List of reached instance sizes of teamA.
        results1: list
            List of reached instance sizes of teamB.
        teamA: int
            Group number of teamA.
        teamB: int
            Group number of teamB.
        Returns:
        ----------
        str:
            The formatted summary message.
    """
    return ""
    summary = ""
    for i in range(int(options.battle_iterations)):
        summary += '='*25
        summary += '\n\nResults of battle {}:\n'.format(i+1)
        if results0[i] == 0:
            summary += 'Solver {} did not solve a single instance.\n'.format(teamA)
        else:
            summary += 'Solver {} solved all instances up to size {}.\n'.format(teamA, results0[i])

        if results1[i] == 0:
            summary += 'Solver {} did not solve a single instance.\n'.format(teamB)
        else:
            summary += 'Solver {} solved all instances up to size {}.\n'.format(teamB, results1[i])

    summary += 'Average solution size of group {}: {}\n'.format(teamA, sum(results0)//int(options.battle_iterations))
    summary += 'Average solution size of group {}: {}\n'.format(teamB, sum(results1)//int(options.battle_iterations))

    return summary

def format_summary_message_averaged(results, all_battle_pairs):
    """ Format the results of a battle into a summary message.

        Parameters:
        ----------
        results0: list 
            List of approximation ratios of teamA for each battle.
        results1: list
            List of approximation ratios of teamB for each battle.
        teamA: int
            Group number of teamA.
        teamB: int
            Group number of teamB.
        Returns:
        ----------
        str:
            The formatted summary message.
    """
    return ""

    summary = ""
    for i in range(int(options.battle_iterations)):
        summary += '='*25
        summary += '\n\nResults of battle {}:\n'.format(i+1)
        if len(results0) > 0:
            summary += 'Average approximation ratio of group {} with {} solved instances: {:.4f}\n'.format(teamA, len(results0[i]), sum(results0[i]) / len(results0[i]))
        else:
            summary += 'Group {} did not give a correct solution for any of the instances of this battle.'.format(teamA)
        if len(results1) > 0:
            summary += 'Average approximation ratio of group {} with {} solved instances: {:.4f}\n'.format(teamB, len(results1[i]), sum(results1[i]) / len(results1[i]))
        else:
            summary += 'Group {} did not give a correct solution for any of the instances of this battle.'.format(teamA)

    return summary


if __name__ == "__main__":
    main()